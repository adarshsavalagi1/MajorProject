import ollama
from utils.db import Chunk
from utils.textbook import TextbookPrompt



class Engine:
    def __init__(self):
        """
        Initialize the Engine and connect to the Ollama client.
        """
        try:
            self.ollama = ollama.Client()  # Instantiate the Ollama client
        except Exception as e:
            print(f"Failed to initialize Ollama client: {e}")
            self.ollama = None

    def is_ready(self) -> bool:
        """
        Check if the engine is ready to process requests.
        Returns True if Ollama client is initialized, False otherwise.
        """
        return self.ollama is not None

    def get_response(self, prompt: TextbookPrompt, prev_chats: list) -> str:
        """
        Generate a response for the given prompt using the Ollama client.

        Args:
            prompt (TextbookPrompt): The prompt to generate a response for.
            prev_chats (list): List of previous chat messages represented as Chunk objects.

        Returns:
            str: The response generated by the model.
        """
        if not self.is_ready():
            return "Engine is not ready. Please check the configuration."

        try:
            # Prepare the conversation context from previous chats (using Chunk's content)
            history = []

            for chunk in prev_chats['previous_responses']:
                history.append({"role": "user", "content": chunk['prompt']})
                # Assuming chunk has a "response" field, if not, adjust accordingly
                history.append({"role": "assistant", "content": chunk['response']}) 

            # Add the current prompt to the context
            history.append({"role": "user", "content": prompt})

         

            # Use the Ollama client to generate the response
            response = self.ollama.chat(
                model="llama3.2:1b",  # Specify the model to use
                messages=history,  # Provide the conversation history
        options={'num_predict': 1000000,'temperature':0.3},
            )

            # Debugging: Print the raw response
            print("Raw response:", response['message']['content'])

            # Ensure response contains the expected structure
            return response['message']['content']

        except Exception as e:
            return f"Error generating response: {e}"


    def close(self):
        """
        Close the Ollama client connection.
        """
        if self.ollama:
            self.ollama.close()

# Example usage
if __name__ == "__main__":
    engine = Engine()
    prompt = TextbookPrompt("Explain Newton's laws of motion.", metadata={"subject": "Physics", "level": "High School"})

    if engine.is_ready():
        print("Engine is ready.")
        response = engine.get_response(prompt)
        print("Response:", response)
    else:
        print("Engine is not ready.")
